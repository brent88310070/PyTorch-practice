{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # plotting library\n",
    "import numpy as np # this module is useful to work with numerical arrays\n",
    "import pandas as pd \n",
    "import random \n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/train-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/train-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'dataset'\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n",
    "test_dataset  = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset.transform = train_transform\n",
    "test_dataset.transform = test_transform\n",
    "\n",
    "m=len(train_dataset)\n",
    "\n",
    "train_data, val_data = random_split(train_dataset, [int(m-m*0.2), int(m*0.2)])\n",
    "batch_size=256\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[0]))\n",
    "print(images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims):  \n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, stride=2, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=0)  \n",
    "        self.linear1 = nn.Linear(3*3*32, 128)\n",
    "        self.linear2 = nn.Linear(128, latent_dims)\n",
    "        self.linear3 = nn.Linear(128, latent_dims)\n",
    "\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
    "        self.N.scale = self.N.scale.cuda()\n",
    "        self.kl = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.batch2(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu =  self.linear2(x)\n",
    "        sigma = torch.exp(self.linear3(x))\n",
    "        z = mu + sigma*self.N.sample(mu.shape)\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "        return z    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(latent_dims, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 3 * 3 * 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder_conv(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = VariationalEncoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VariationalAutoencoder(\n",
       "  (encoder): VariationalEncoder(\n",
       "    (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (batch2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (linear1): Linear(in_features=288, out_features=128, bias=True)\n",
       "    (linear2): Linear(in_features=128, out_features=4, bias=True)\n",
       "    (linear3): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_lin): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=128, out_features=288, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (unflatten): Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
       "    (decoder_conv): Sequential(\n",
       "      (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): ConvTranspose2d(8, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "d = 4\n",
    "\n",
    "vae = VariationalAutoencoder(latent_dims=d)\n",
    "\n",
    "lr = 1e-3 \n",
    "\n",
    "optim = torch.optim.Adam(vae.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training function\n",
    "def train_epoch(vae, device, dataloader, optimizer):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    vae.train()\n",
    "    train_loss = 0.0\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for x, _ in dataloader: \n",
    "        # Move tensor to the proper device\n",
    "        x = x.to(device)\n",
    "        x_hat = vae(x)\n",
    "        # Evaluate loss\n",
    "        loss = ((x - x_hat)**2).sum() + vae.encoder.kl\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print batch loss\n",
    "        print('\\t partial train loss (single batch): %f' % (loss.item()))\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "    return train_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_epoch(vae, device, dataloader):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    vae.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        for x, _ in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            x = x.to(device)\n",
    "            # Encode data\n",
    "            encoded_data = vae.encoder(x)\n",
    "            # Decode data\n",
    "            x_hat = vae(x)\n",
    "            loss = ((x - x_hat)**2).sum() + vae.encoder.kl\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('transformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa6f6b011c3f9206a9d65ad3e68186feab4e9bf9a494de8a98686188662c05b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
